{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ddfe53a-c6bc-4678-abc6-d9285b1fbb68",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68011878-0136-4e8d-a2e0-1d4986fdbf4a",
   "metadata": {},
   "source": [
    "%cd ../..\n",
    "import torch\n",
    "from torch import nn, Tensor, optim\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from dlbpgm.datasets.preprocessing import TrayPreprocessor, RGBVIFilter, ImageToLatentConverter\n",
    "from dlbpgm.datasets.metadata import Metadata"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4aedce2e-0b6b-4b89-b969-eb788fc07d05",
   "metadata": {},
   "source": [
    "## Crop Plants\n",
    "In the original dataset the images of the plants are provided in the form of 4 high-resolution images of trays.\n",
    "We cut out each individual plant from these trays in an area of $512^2$ px.\n",
    "The coordinates of the respective plants can be imported from the `dlbpgm` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6418bd5-183a-4367-be14-84342e6c31b7",
   "metadata": {},
   "source": [
    "from dlbpgm.datasets.preprocessing import tray_grids\n",
    "tray_grids"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9e3a017a-3a21-407a-9e3d-0ce9a062854b",
   "metadata": {},
   "source": [
    "In a next step we list up source and target directories of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d656ef60-306c-4803-ba14-37353aa27cd3",
   "metadata": {},
   "source": [
    "trays_dirs = [\n",
    "    Path(\"Arabitopsis/images_and_annotations/PSI_Tray031/tv/\"),\n",
    "    Path(\"Arabitopsis/images_and_annotations/PSI_Tray032/tv/\"),\n",
    "    Path(\"Arabitopsis/images_and_annotations/PSI_Tray033/tv/\"),\n",
    "    Path(\"Arabitopsis/images_and_annotations/PSI_Tray034/tv/\"),\n",
    "]\n",
    "\n",
    "target_dirs = [\n",
    "    Path(\"Arabidopsis/plants/1\"),\n",
    "    Path(\"Arabidopsis/plants/2\"),\n",
    "    Path(\"Arabidopsis/plants/3\"),\n",
    "    Path(\"Arabidopsis/plants/4\"),\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4e465134-0adf-409e-95f0-3d9910db8e12",
   "metadata": {},
   "source": [
    "We will iteratively configure a `TrayPreprocessor` Object from the `dlbpgm` package for each tray, which we will further refer to by an index value `i`.\n",
    "\n",
    "The `TrayPreprocessor` needs to be specified with a source and a target directory, as well as the crop size and the plant positions, coming from the imported coordinates.\n",
    "The tray number is only passed to name the outcoming images appropriately.\n",
    "As a last argument, we pass a `RGBVIFilter` which is resposible for the separation of fore- and background in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eacbd512-1a6f-4c32-a1e9-41a9af9b88d6",
   "metadata": {},
   "source": [
    "i = 2\n",
    "\n",
    "preprocessor = TrayPreprocessor(\n",
    "    source_dir=trays_dirs[i],\n",
    "    target_dir=target_dirs[i],\n",
    "    tray_number=i+1, \n",
    "    crop_size=512, \n",
    "    grid=tray_grids[i], \n",
    "    rgbvi_filter=RGBVIFilter()\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cbd064c-92cf-4178-beca-e86235dd618b",
   "metadata": {},
   "source": [
    "preprocessor.process_files()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9ffd0fe6-8721-4175-a7f6-05b1b6057be2",
   "metadata": {},
   "source": [
    "## Pre-calculate Latent Representations\n",
    "In order to improve training speed, we pre-calculate the latent representations of the images.\n",
    "For this, we use the `ImageToLatentConverter` from the `dlbpgm` package.\n",
    "\n",
    "To speed up conversion, in a first step we set up a GPU device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10138789-d7f5-4528-b47f-2b865e075bbe",
   "metadata": {},
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1554cb5e-c8a9-49b2-afbb-a4827dc8da4a",
   "metadata": {},
   "source": [
    "In a next step we load the metadata of the dataset and specify a location, where the latent variables should be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb132fb2-8fb5-4b23-bcce-5c5e4a46f59c",
   "metadata": {},
   "source": [
    "metadata = Metadata.load(\"Arabidopsis/metadata.csv\")\n",
    "latents_dir = Path(\"Arabidopsis/latents128\")\n",
    "metadata['latent_path'] = metadata.latent_path.apply(lambda p: latents_dir/p)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d53bcc24-f693-4c33-ab94-33372a22d35b",
   "metadata": {},
   "source": [
    "To convert images into latent variables, we use the `ImageToLatentConverter` from the `dlbpgm` package. \n",
    "In order to use this tool, we provide it with the metadata, batch size, and number of workers as input parameters. \n",
    "Additionally, we have the option to specify any transformations that should be applied to the images before the conversion process, by using the `transforms` parameter. \n",
    "This includes resizing images to the desired size or applying augmentation techniques if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f1a682b-b91c-453b-8273-2d94c24a81a9",
   "metadata": {},
   "source": [
    "image_size = 128\n",
    "\n",
    "converter = ImageToLatentConverter(\n",
    "    metadata, \n",
    "    batch_size=16, \n",
    "    num_workers=4,\n",
    "    transforms=T.Compose([\n",
    "        T.Lambda(lambda t: t[:, :3] * t[:, 3:]), # RGBA -> RGB * A\n",
    "        # T.RandomVerticalFlip(p=1.0),\n",
    "        T.Resize((image_size, image_size), antialias=True),\n",
    "    ])\n",
    ").to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a280e50-ed0d-483d-b754-3190ccc337ab",
   "metadata": {},
   "source": [
    "converter.convert_images()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc09ec-5b41-4a47-b67f-93d03e29cfec",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
