{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8beb19-4f40-47ab-989d-c4700d641e7b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5147cf-84e4-432d-9073-61d5dbf535fa",
   "metadata": {},
   "source": [
    "%cd ../..\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, Tensor, optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from dlbpgm.models import LGU, LGT, LGR\n",
    "from dlbpgm.models.evaluator import Metrics, Evaluator\n",
    "from dlbpgm.datasets import StridedPlantDataset, Metadata, MultiplePlantDataset\n",
    "\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "600b9691-59ac-4e0a-a94c-1d1ac824e07b",
   "metadata": {},
   "source": [
    "## Test Dataset\n",
    "The evaluation of the models was conducted on three different step sizes between the timestamps, that were passed to the models.\n",
    "For simplicity, these were defined as strides within the PyTorch datasets.\n",
    "\n",
    "As with all other examples, we begin by loading the metadata of our dataset, which allows us to access the images in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f457bef-b61e-44a7-864c-4098525eb864",
   "metadata": {},
   "source": [
    "metadata = Metadata.load(\"dlbpgm/resources/metadata.csv\")\n",
    "metadata = metadata.remove_empty_pots()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3c4b74ec-6ad1-400c-b306-a7f4fedfef7b",
   "metadata": {},
   "source": [
    "To generate a PyTorch dataset from the metadata, we make use of the two classes `StridedPlantDataset` and `MultiplePlantDataset`.\n",
    "\n",
    "The `StridedPlantDataset` class manages a time series of a single plant and produces samples in the form of images and their corresponding timestamps. \n",
    "When initializing the class, additional keyword arguments are required to determine the sequence lengths, which specifies the number of images in each sample. \n",
    "We also specify the stride between the timestamps used for sampling and transforms to be applied to the data before feeding them into the models.\n",
    "In this case, we separate for- and background of the images, using the alpha-channel of the images as a mask.\n",
    "Additionally, we scale each image to a resolution of $256^2$ px.\n",
    "\n",
    "`MultiplePlantDataset` is a wrapper class that creates a `StridedPlantDataset` object for each plant contained in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e47e949-760c-424a-aeff-71883734f65b",
   "metadata": {
    "tags": []
   },
   "source": [
    "stride = 40  # models were evaluated with stride 40, 80, and 150\n",
    "\n",
    "test_ds = MultiplePlantDataset(\n",
    "    metadata=metadata.test_ds(),\n",
    "    dataset_type=StridedPlantDataset,\n",
    "    stride=stride,\n",
    "    sequence_length=9,\n",
    "    transform=T.Compose([\n",
    "        T.Lambda(lambda t: t[:, :3] * t[:, 3:]),\n",
    "        T.Resize((256, 256), antialias=True),\n",
    "    ])\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fd1de2ca-dc49-4ab9-aca9-4717eae3044c",
   "metadata": {},
   "source": [
    "## Model\n",
    "This this case, we evaluate the LGU model on a resolution of $256^2$ px.\n",
    "In a first step, we initialize the model by specifying the desired resolution and the path to the pre-trained model weights.\n",
    "(It may happen that you need to adjust the checkpoint path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e404ac-06f2-4c6a-ab6c-4f5f3fea0c12",
   "metadata": {},
   "source": [
    "model = LGU(\n",
    "    resolution=256, \n",
    "    ckpt_path=\"Arabidopsis/experiments/256x256/LGUnet/lgu256.pt\"\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "77efa887-8876-4be3-93b9-c9f30d8329bb",
   "metadata": {},
   "source": [
    "The next step includes the initialization of the `Evaluator` by passing model, test dataset, batch size, and number of workers to the constructor.\n",
    "If possible, we also put the evaluator on the GPU to speed up the evaluation and set it to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "935aa55b-de9a-456d-8933-5f60a2b86745",
   "metadata": {
    "tags": []
   },
   "source": [
    "evaluator = Evaluator(\n",
    "    model=model,\n",
    "    dataset=test_ds,\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    ").to(device).eval()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2571135c-4531-4573-9235-8804dd49e6d2",
   "metadata": {},
   "source": [
    "Now we are ready to start the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe42b59-3597-4f5c-8118-b0b13c8a686f",
   "metadata": {
    "tags": []
   },
   "source": [
    "evaluator.evaluate()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "13535ace-5faa-44d0-a22b-80275fc21b48",
   "metadata": {},
   "source": [
    "The results are returned on form of a tuple, containing three dictionaries.\n",
    "The first one contains the metrics measured on both interpolated and extrapolated predictions.\n",
    "The second and third dictionaries look at the two cases serparately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68668a01-6cd8-4e10-9d3e-67f0812e89b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "metrics = evaluator.compute_metrics()\n",
    "metrics"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22232297-9347-4ee3-a771-08f66438b170",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
