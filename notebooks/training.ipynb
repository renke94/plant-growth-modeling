{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c3da3d-c395-40b2-9e49-9ba441adc4d8",
   "metadata": {},
   "source": [
    "# Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc61440-f1d4-4044-9124-f00f058f16cd",
   "metadata": {},
   "source": [
    "%cd ../..\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, Tensor, optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from dlbpgm.datasets import Metadata, PlantDataModule, PlantMetaDataset, PlantDataset, PlantLatentDataset, MultiplePlantDataset\n",
    "from dlbpgm.models import LGMTrainer, LGUnet, LatentGrowthTransformer, LatentGrowthRegression\n",
    "from dlbpgm.models.lgm import LatentGrowthVAE, LossDict\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5d65411d-ec5e-4980-83eb-9191071f7cb3",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We use metadata objects to organize our datasets. \n",
    "These objects are an extension of the Pandas DataFrame and contain helpful functions for our use case. \n",
    "\n",
    "All metadata is saved in a CSV file and can be easily loaded from the Metadata class.\n",
    "The CSV files contain all relevant information on the plants in the dataset.\n",
    "This includes the plant's identification, file paths to its images and latent variables, timestamps, and whether the plant exists on the image. It also specifies whether the plant belongs to the training, validation, or test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2988c59-0425-4e08-bd09-31ed0b6159d7",
   "metadata": {},
   "source": [
    "metadata = Metadata.load(\"dlbpgm/resources/metadata.csv\")\n",
    "metadata = metadata.query(\"plant_exists == True\")\n",
    "metadata.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "990dd7f2-ca8a-4e98-bdf3-e450e067ff9b",
   "metadata": {},
   "source": [
    "In the following we split the data into train-, validation-, and test data.\n",
    "\n",
    "To generate PyTorch datasets from the metadata, we make use of the two classes `PlantLatentDataset` and `MultiplePlantDataset`.\n",
    "\n",
    "The `PlantLatentDataset` class manages a time series of a single plant and produces samples in the form of latent variables. \n",
    "When initializing the class, additional keyword arguments are required to determine the number of inputs and targets to sample from the time series. \n",
    "We also specify the size of the time window used for sampling and the path to the parent directory of the latent variables.\n",
    "\n",
    "`MultiplePlantDataset` is a wrapper class that creates a `PlantLatentDataset` object for each plant contained in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8dba7b3-226b-4861-8f25-42f05df996d1",
   "metadata": {},
   "source": [
    "# costumize path to the location where your latents are stored\n",
    "path_to_latents = Path(\"Arabidopsis/latents128/\") \n",
    "\n",
    "train_ds = MultiplePlantDataset(\n",
    "    metadata=metadata.train_ds(),\n",
    "    dataset_type=PlantLatentDataset,\n",
    "    timespan=400,\n",
    "    n_inputs=4,\n",
    "    n_targets=3,\n",
    "    latents_dir=path_to_latents\n",
    ")\n",
    "\n",
    "val_ds = MultiplePlantDataset(\n",
    "    metadata=metadata.val_ds(),\n",
    "    dataset_type=PlantLatentDataset,\n",
    "    timespan=400,\n",
    "    n_inputs=4,\n",
    "    n_targets=3,\n",
    "    latents_dir=path_to_latents\n",
    ")\n",
    "\n",
    "test_ds = MultiplePlantDataset(\n",
    "    metadata=metadata.test_ds(),\n",
    "    dataset_type=PlantLatentDataset,\n",
    "    timespan=400,\n",
    "    n_inputs=4,\n",
    "    n_targets=3,\n",
    "    latents_dir=path_to_latents\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, num_workers=4, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=64, num_workers=4, shuffle=None)\n",
    "test_dl  = DataLoader(test_ds, batch_size=64, num_workers=4, shuffle=None)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e644d8-ae1a-4f0c-9f91-2c329a9559a2",
   "metadata": {},
   "source": [
    "# Example variables for inference after training\n",
    "z_in, t_in, t_out, z_out =  next(iter(DataLoader(test_ds, batch_size=4, shuffle=True)))\n",
    "\n",
    "kwargs = dict(\n",
    "    z_in=z_in.to(device), z_out=z_out.to(device),\n",
    "    t_in=t_in.to(device), t_out=t_out.to(device),\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6f822f72-533d-4f97-8cf3-7c938ee05f05",
   "metadata": {},
   "source": [
    "## Model\n",
    "In the following, the models `LGT`, `LGT`, and `LGU` are initialized for an image resolution of $128^2$ px.\n",
    "\n",
    "The model configurations are identical to those used in the referenced work and can be adjusted as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4412f754-9325-44e2-b034-86c0cec159ac",
   "metadata": {},
   "source": [
    "image_size = 128\n",
    "latent_size = image_size // 8\n",
    "\n",
    "# LGT\n",
    "lgt = LatentGrowthTransformer(\n",
    "    resolution=latent_size,\n",
    "    nhead=32,\n",
    "    dim_head=16,\n",
    "    dropout=0.1,\n",
    "    n_positions=1000\n",
    ")\n",
    "\n",
    "# LGR\n",
    "lgr = LatentGrowthRegression(\n",
    "    resolution=latent_size,\n",
    "    nhead=32,\n",
    "    dim_head=16,\n",
    "    dropout=0.1,\n",
    "    n_positions=1000\n",
    ")\n",
    "\n",
    "# LGU\n",
    "lgu = LGUnet(\n",
    "    input_shape=(4, latent_size, latent_size),\n",
    "    down_features=[128, 256, 512],\n",
    "    up_features=[512, 256, 256],\n",
    "    nhead=32,\n",
    "    dim_head=16,\n",
    "    dropout=0.1,\n",
    "    n_positions=1000\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7f6805d8-73ef-4f63-b63f-85248abf28ae",
   "metadata": {},
   "source": [
    "## Training\n",
    "The training is carried out with the class `LGMTrainer`.\n",
    "This requires that we specify a path in which model checkpoints can be saved during training.\n",
    "In addition, we pass the model to be trained and configure the learning rate and the gradient accumulation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c4e71e1-e4d0-46c5-b888-8ac9148c6e2f",
   "metadata": {},
   "source": [
    "# create a folder where to save checkpoints during training\n",
    "experiment_dir = Path(\"LGM\")\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5b49d9b-1d4e-41da-91ee-7a401c0f9dd5",
   "metadata": {},
   "source": [
    "trainer = LGMTrainer(\n",
    "    path=experiment_dir,\n",
    "    model=lgt,  # pass lgt, lgr, or lgu, depending on which model you want to train\n",
    "    lr=1e-4,\n",
    "    gradient_accumulation_steps=1\n",
    ").to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a6c0e-fb8b-4ac3-ab8f-602011ec614a",
   "metadata": {},
   "source": [
    "for i in range(100):\n",
    "    trainer.train_epoch(train_dl)\n",
    "    trainer.validation_epoch(val_dl)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "85260a20-36e4-4111-8483-9a947b33e032",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Since we are carrying out the training in latent space, we need the decoder of the pre-trained VAE, which transforms the latent variables into images.\n",
    "This is initialized in the first step.\n",
    "\n",
    "In the next step, we transform both inputs and predictions into images, which we can then plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8da494cc-25fa-4193-a70a-2ceffc27296e",
   "metadata": {},
   "source": [
    "# pre-trained VAE to decode latent representations to images\n",
    "vae = LatentGrowthVAE().to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "789dc079-3897-44a0-96d8-3bfaee2e801f",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    # decode input latents to images\n",
    "    _x = vae.decode(kwargs['z_in'])\n",
    "    x_in = _x.cpu()\n",
    "\n",
    "    # inference in latent space\n",
    "    trainer.model.eval()\n",
    "    _x, _ = trainer.model(**kwargs)\n",
    "\n",
    "    # decode predicted latents to images\n",
    "    _x = vae.decode(_x)\n",
    "    x_pred = _x.cpu()\n",
    "    del _x\n",
    "\n",
    "x = torch.cat([x_in, x_pred], dim=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06c505e6-64c7-4f89-a39f-95c078799347",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "grid_img = make_grid(x.flatten(0, 1), nrow=7)\n",
    "plt.imshow(grid_img.permute(1, 2, 0))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23b59d-7f8f-4cd9-a4b1-48e434744ea3",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
